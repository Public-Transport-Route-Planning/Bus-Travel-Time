{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\66898\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import seaborn as sns\n",
    "import glob\n",
    "import math\n",
    "from datetime import datetime\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n",
    "\n",
    "label_encoder = LabelEncoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "route_path = \"data/csv/relationship_routes.csv\"\n",
    "relationship_route = pd.read_csv(route_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'process-time.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df_process \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprocess-time.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m missing_rows \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmissing_rows.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m df_routes_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([df_process, missing_rows], ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\66898\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    945\u001b[0m )\n\u001b[0;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\66898\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\66898\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\66898\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1706\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1711\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1712\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1714\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\66898\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    859\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    860\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    862\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    865\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    866\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    867\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    868\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'process-time.csv'"
     ]
    }
   ],
   "source": [
    "df_process = pd.read_csv(\"process-time.csv\")\n",
    "missing_rows = pd.read_csv(\"missing_rows.csv\")\n",
    "df_routes_data = pd.concat([df_process, missing_rows], ignore_index=True)\n",
    "\n",
    "df_routes_data[\"ts1\"] = pd.to_datetime(df_routes_data[\"ts1\"])\n",
    "df_routes_data[\"ts2\"] = pd.to_datetime(df_routes_data[\"ts2\"])\n",
    "\n",
    "# def get_day_of_week(timestamp):\n",
    "#     return timestamp.day_name()\n",
    "\n",
    "df_routes_data[(df_routes_data['route_id'] == '1-38') & (~df_routes_data['mins'].isna())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_routes_data.drop(columns=[\"lat\", \"lon\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_routes_data = df_routes_data[df_routes_data['route_id'] == '2-23']\n",
    "\n",
    "df_routes_data['route_id_encoded'] = label_encoder.fit_transform(df_routes_data['route_id'])\n",
    "\n",
    "df_routes_data['weekend'] = np.where((df_routes_data['day_of_week'] == 'Saturday') | (df_routes_data['day_of_week'] == 'Sunday'), True, False)\n",
    "\n",
    "day_to_num = {\n",
    "    \"Monday\": 1,\n",
    "    \"Tuesday\": 2,\n",
    "    \"Wednesday\": 3,\n",
    "    \"Thursday\": 4,\n",
    "    \"Friday\": 5,\n",
    "    \"Saturday\": 6,\n",
    "    \"Sunday\": 7,\n",
    "}\n",
    "\n",
    "# def map_time_periods(hour):\n",
    "#     if 4 <= hour <= 8:\n",
    "#         return 0\n",
    "#     elif 8 <= hour <= 10:\n",
    "#         return 1\n",
    "#     elif 10 <= hour <= 12:\n",
    "#         return 0\n",
    "#     elif 12 <= hour <= 15:\n",
    "#         return 0\n",
    "#     elif 15 <= hour <= 20:\n",
    "#         return 1\n",
    "#     else:\n",
    "#         return 0\n",
    "def map_time_periods(hour):\n",
    "    if 6 <= hour <= 10:  # Merged the first two conditions as they return the same value\n",
    "        return 'peak'\n",
    "    elif 10 < hour <= 12:\n",
    "        return 'off-peak'\n",
    "    elif 12 < hour <= 15:\n",
    "        return 'off-peak'\n",
    "    elif 15 < hour <= 20:\n",
    "        return 'peak'\n",
    "    else:\n",
    "        return 'off-peak'\n",
    "\n",
    "df_routes_data[\"time_periods\"] = df_routes_data[\"hrs\"].apply(map_time_periods)\n",
    "\n",
    "df_routes_data[\"day_of_week\"] = df_routes_data[\"day_of_week\"].map(day_to_num)\n",
    "\n",
    "df_routes_data.sort_values(by=[\"seq\", \"day_of_week\", \"time_periods\"], inplace=True)\n",
    "\n",
    "df_routes_data = df_routes_data.reset_index(drop=True)\n",
    "df_routes_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "weight travel time for each neighboring node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def linear_weight(t_diff):\n",
    "#     return 1 / (1 + t_diff)\n",
    "\n",
    "# # Calculate time difference\n",
    "# df['time_difference_k'] = (pd.to_datetime(df['ts1'].dt.strftime('%H:%M:%S')) - pd.to_datetime(df['hrs'].astype(str) + ':00')).dt.total_seconds() / 60\n",
    "\n",
    "# # Calculate weights using the linear weight function\n",
    "# df['weight_k'] = df['time_difference_k'].apply(linear_weight)\n",
    "\n",
    "# # Calculate weighted travel time for each neighboring node\n",
    "# df['weighted_travel_time_k'] = df['weight_k'] * df['mins']\n",
    "\n",
    "# # # Calculate the numerator and denominator for the final calculation\n",
    "# numerator = df.groupby(['sid1', 'sid2', 'route_id'])['weighted_travel_time_k'].sum()\n",
    "# denominator = df.groupby(['sid1', 'sid2', 'route_id'])['weight_k'].sum()\n",
    "\n",
    "# df_result = numerator / denominator\n",
    "\n",
    "# df = pd.merge(df, df_result.reset_index(), on=['sid1', 'sid2', 'route_id'], how='left')\n",
    "\n",
    "# df.rename(columns={0:'weighted_travel_time_route'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "historical average travel time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# filtered_df = df[(df['sid1'] == 4151) & (df['sid2'] == 4142) & (df['route_id'] == '2-23')]\n",
    "\n",
    "# historical_average = filtered_df['mins'].mean()\n",
    "\n",
    "# print(f\"The historical average travel time between stops i and j is {historical_average} minutes.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2 = df[(df[\"route_id\"] == '2-16') & (df[\"sid1\"] == 3851) & (df[\"sid2\"] == 3776) & (df[\"hrs\"] == 4)]\n",
    "\n",
    "# for bus in df2['vid'].unique():\n",
    "#    plt.plot(df2[df2['vid'] == bus]['mins'], marker='', linestyle='-', label=bus)\n",
    "\n",
    "\n",
    "# plt.xlabel('ts1')\n",
    "# plt.ylabel('mins')\n",
    "# plt.title('Line Graph Example')\n",
    "# plt.legend()\n",
    "\n",
    "# df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2 = df.reset_index()[\"mins\"]\n",
    "# plt.plot(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tranformation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = adfuller(df[\"mins\"])\n",
    "# print(\"ADF Statistic: %f\" % result[0])\n",
    "# print(\"p-value: %f\" % result[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[\"mins_diff\"] = df[\"mins\"].diff()\n",
    "# df = df.dropna()  # Differencing leads to NaN values for the first row, so drop it\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[\"mins_log\"] = np.log(df[\"mins\"])\n",
    "# df[\"mins_log_diff\"] = df[\"mins_log\"].diff()\n",
    "# df = df.dropna()\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seasonal_difference = df[\"mins\"].diff(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seasonal_difference = df[\"mins_diff\"].diff(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = adfuller(df[\"mins_log_diff\"].dropna(), autolag=\"AIC\")\n",
    "\n",
    "# print('ADF Statistic:', result[0])\n",
    "# print(\"p-value:\", result[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_routes_data[(~df_routes_data[\"mins\"].isna())]\n",
    "df_test = df_routes_data[(df_routes_data[\"mins\"].isna())][:10000]\n",
    "\n",
    "# Feature selection\n",
    "features = [\n",
    "    \"sid1\",\n",
    "    \"sid2\",\n",
    "    # \"speed\",\n",
    "    \"route_id_encoded\",\n",
    "    \"direction\",\n",
    "    \"seq\",\n",
    "    \"hrs\",\n",
    "    \"day_of_week\",\n",
    "    \"weekend\",\n",
    "    \"time_periods\",\n",
    "    # \"weighted_travel_time_route\",\n",
    "]\n",
    "target = \"mins\"\n",
    "\n",
    "#################################################\n",
    "# Select features and target variable\n",
    "X = df[features]\n",
    "y = df[target]\n",
    "\n",
    "# X['route_id_encoded'] = label_encoder.transform(X['route_id'])\n",
    "\n",
    "X_encoded = pd.get_dummies(\n",
    "    X,\n",
    "    columns=[\n",
    "        \"direction\",\n",
    "        \"time_periods\",\n",
    "    ],\n",
    "    drop_first=True,\n",
    ")\n",
    "\n",
    "#################################################\n",
    "\n",
    "X_new = df_test[features]\n",
    "y_new = df_test[\"mins\"]\n",
    "\n",
    "X_new_encoded = pd.get_dummies(\n",
    "    X_new,\n",
    "    columns=[\n",
    "        \"direction\",\n",
    "        \"time_periods\",\n",
    "    ],\n",
    "    drop_first=True,\n",
    ")\n",
    "\n",
    "###################################################\n",
    "\n",
    "X_encoded\n",
    "# X_new_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import cross_val_score\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# # Assuming X is your feature matrix and y is the continuous target variable\n",
    "# model = RandomForestRegressor()\n",
    "\n",
    "# # Perform K-Fold Cross-validation for Regression\n",
    "# cv_results = cross_val_score(model, X_encoded, y, cv=8, scoring='neg_mean_squared_error')\n",
    "\n",
    "# # Print the average mean squared error\n",
    "# print(f'Average Mean Squared Error: {-cv_results.mean()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True, feat_name=None):\n",
    "\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [f\"{feat_name[j]}(t-{i})\" for j in range(n_vars)]\n",
    "\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [f\"{feat_name[j]}(t)\" for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [f\"{feat_name[j]}(t+{i})\" for j in range(n_vars)]\n",
    "\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reframed = series_to_supervised(X_encoded.values, 30, 3, feat_name=X_encoded.columns)\n",
    "# reframed.drop(['High(t)','High(t+1)','High(t+2)','Low(t)','Low(t+1)','Low(t+2)','Open(t)','Open(t+1)','Open(t+2)','Volume(t)','Volume(t+1)',\\\n",
    "#                'Volume(t+2)'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train-Test Split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_encoded, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standradize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the data\n",
    "# scaler_X = StandardScaler()\n",
    "scaler_X = MinMaxScaler(feature_range=(0, 1))\n",
    "X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "X_test_scaled = scaler_X.transform(X_test)\n",
    "\n",
    "# scaler_y = StandardScaler()\n",
    "scaler_y = MinMaxScaler(feature_range=(0, 1))\n",
    "y_train_scaled = scaler_y.fit_transform(np.array(y_train).reshape(-1, 1))\n",
    "y_test_scaled = scaler_y.transform(np.array(y_test).reshape(-1, 1))\n",
    "\n",
    "y_test_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_new_scaled = scaler_X.transform(X_new_encoded)\n",
    "\n",
    "# # scaler_y = StandardScaler()\n",
    "# # y_new_scaled = scaler_y.fit_transform(np.array(y_new).reshape(-1, 1))\n",
    "# # y_new_scaled = scaler_y.transform(np.array(y_new).reshape(-1, 1))\n",
    "# X_new_scaled.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshape data for LSTM input (assuming sequence length of 1)\n",
    "\n",
    "reshape input to be 3D [samples, timesteps, features]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_lstm = X_train_scaled.reshape(\n",
    "    (X_train_scaled.shape[0], 1, X_train_scaled.shape[1])\n",
    ")\n",
    "X_test_lstm = X_test_scaled.reshape(\n",
    "    (X_test_scaled.shape[0], 1, X_test_scaled.shape[1]))\n",
    "\n",
    "X_train_lstm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_new_lstm = X_new_scaled.reshape(\n",
    "#     (X_new_scaled.shape[0], 1, X_new_scaled.shape[1]))\n",
    "\n",
    "# X_new_lstm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating and fitting LSTM model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# design network\n",
    "model = Sequential()\n",
    "model.add(\n",
    "    LSTM(\n",
    "        128,\n",
    "        return_sequences=True,\n",
    "        activation=\"relu\",\n",
    "        input_shape=(X_train_lstm.shape[1], X_test_lstm.shape[2]),\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "model.add(LSTM(128, return_sequences=True))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(1, activation='relu'))\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM with drop and Dense('relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.models import Model\n",
    "# from tensorflow.keras.layers import Input, LSTM, Dense, Add, Activation, Dropout\n",
    "# from tensorflow.keras import regularizers\n",
    "\n",
    "# def create_residual_model(input_shape):\n",
    "#     inputs = Input(shape=input_shape)\n",
    "    \n",
    "#     # Initial LSTM layer\n",
    "#     x = LSTM(128, return_sequences=False)(inputs)\n",
    "#     x = Dropout(0.5)(x)  # Apply dropout\n",
    "#     # First dense block\n",
    "#     x_shortcut = x\n",
    "#     x = Dense(1, activation='relu', kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "#     x = Dense(1, kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "    \n",
    "#     # Add residual connection\n",
    "#     x = Add()([x, x_shortcut])\n",
    "#     x = Activation('relu')(x)\n",
    "    \n",
    "#     # Additional dense blocks as needed with residual connections\n",
    "#     for _ in range(3 - 1):  # Define your desired number of residual blocks\n",
    "#         x_shortcut = x\n",
    "#         x = Dense(1, activation='relu', kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "#         x = Dense(1, kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "        \n",
    "#         # Add residual connection\n",
    "#         x = Add()([x, x_shortcut])\n",
    "#         x = Activation('relu')(x)\n",
    "\n",
    "\n",
    "#     # x = BatchNormalization()(x)\n",
    "#     # Output layer\n",
    "#     outputs = Dense(1, activation='relu')(x)  # Assuming a regression task\n",
    "    \n",
    "#     model = Model(inputs=inputs, outputs=outputs)\n",
    "#     return model\n",
    "\n",
    "# input_shape = (X_train_lstm.shape[1], X_train_lstm.shape[2])  # Replace with your input shape\n",
    "# model = create_residual_model(input_shape)\n",
    "\n",
    "# model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM with drop and Dense('tanh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.models import Model\n",
    "# from tensorflow.keras.layers import Input, LSTM, Dense, Add, Activation, Dropout\n",
    "# from tensorflow.keras import regularizers\n",
    "\n",
    "# def create_advanced_model(input_shape):\n",
    "#     inputs = Input(shape=input_shape)\n",
    "    \n",
    "#     # Initial LSTM layer\n",
    "#     x = LSTM(128, return_sequences=False)(inputs)\n",
    "#     # Apply dropout\n",
    "\n",
    "#     # First dense block with residual connection\n",
    "#     x_shortcut = x\n",
    "#     x = Dense(1, activation='relu', kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "#     x = Dense(1, kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "    \n",
    "#     # Add residual connection\n",
    "#     x = Add()([x, x_shortcut])\n",
    "#     x = Activation('relu')(x)\n",
    "    \n",
    "#     # Additional dense blocks with dropout, L2 regularization, and residual connections\n",
    "#     for _ in range(number_of_residual_blocks - 1):  # Define the desired number of residual blocks\n",
    "#         x_shortcut = x\n",
    "#         x = Dense(1, activation='relu', kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "#         x = Dense(1, kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "        \n",
    "#         # Add residual connection\n",
    "#         x = Add()([x, x_shortcut])\n",
    "#         x = Activation('relu')(x)\n",
    "        \n",
    "#     x = BatchNormalization()(x)\n",
    "#     # Output layer with tanh activation\n",
    "#     outputs = Dense(1, activation='tanh')(x)\n",
    "    \n",
    "#     model = Model(inputs=inputs, outputs=outputs)\n",
    "#     return model\n",
    "\n",
    "# input_shape = (X_train_lstm.shape[1], X_train_lstm.shape[2])  # Replace with your input shape\n",
    "# number_of_residual_blocks = 3  # Define the number of residual blocks you desire\n",
    "# model = create_advanced_model(input_shape)\n",
    "\n",
    "# model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define call back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "cp = ModelCheckpoint('model_LSTM_drop/', save_best_only=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    X_train_lstm,\n",
    "    y_train_scaled,\n",
    "    epochs=50,\n",
    "    batch_size=64,\n",
    "    validation_data=(X_test_lstm, y_test_scaled),\n",
    "    verbose=1,\n",
    "    callbacks=[\n",
    "        # early_stopping, \n",
    "               cp\n",
    "               ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# พล็อตค่า train&test loss\n",
    "plt.plot(history.history[\"loss\"], label=\"train\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"test\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction and checking performance matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('model_LSTM_drop/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_predictions_scaled = model.predict(X_test_lstm)\n",
    "new_predictions_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # transform to original form\n",
    "# scaler_y = StandardScaler()\n",
    "# y_train_scaled = scaler_y.fit_transform(np.array(y_train).reshape(-1, 1))\n",
    "# y_test_scaled = scaler_y.transform(np.array(y_test).reshape(-1, 1))\n",
    "\n",
    "# new_predictions_scaled = np.maximum(new_predictions_scaled, 0)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predictions = scaler_y.inverse_transform(new_predictions_scaled)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Invert transformation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # invert diff\n",
    "# predictions_inverted_diff = np.cumsum(predictions)\n",
    "# predictions_inverted_diff.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # invert log\n",
    "# predictions_inverted_log = np.exp(predictions_inverted_diff)\n",
    "# predictions_inverted_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # y_new = np.nan_to_num(y_new, nan=0.0, posinf=np.finfo(np.float32).max)\n",
    "# predictions_inverted_log = np.nan_to_num(predictions_inverted_log, nan=0.0, posinf=np.finfo(np.float32).max)\n",
    "# predictions_inverted_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = model.evaluate(X_test_lstm, y_test_scaled, verbose=1)\n",
    "print(f\"Test Loss: {loss:.4f}\")\n",
    "# print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now its time for calculating the rmse performance matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "mae = mean_absolute_error(y_test, predictions)\n",
    "r2 = r2_score(y_test, predictions)\n",
    "print(f\"mean_squared_error: {mse}\")\n",
    "print(f\"Mean Absolute Error: {mae}\")\n",
    "print(f\"R-squared: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_test, predictions, color=\"blue\", label=\"Predictions\")\n",
    "plt.plot(\n",
    "    [y_test.min(), y_test.max()],\n",
    "    [y_test.min(), y_test.max()],\n",
    "    \"k--\",\n",
    "    lw=2,\n",
    "    color=\"red\",\n",
    "    label=\"Perfect Prediction\",\n",
    ")\n",
    "plt.xlabel(\"True Values (y_test)\")\n",
    "plt.ylabel(\"Predictions\")\n",
    "plt.title(\"True vs. Predicted Values\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Invert tranformation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_encoded[\"route_id_encoded\"] = label_encoder.inverse_transform(X_encoded[\"route_id_encoded\"])\n",
    "\n",
    "# X_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(\n",
    "    {\n",
    "        \"Actual\": y_test,\n",
    "        \n",
    "        \"Predicted\": abs(predictions.flatten()),\n",
    "        # \"sid1\": X_encoded[\"sid1\"].values,\n",
    "        # \"sid2\": X_encoded[\"sid2\"].values,\n",
    "        # \"route_id\": X_encoded[\"route_id_encoded\"].values,\n",
    "        # \"Direction\": X_new_encoded[\"direction_go\"].values,\n",
    "        # \"hrs\": X_new_encoded[\"hrs\"].values,\n",
    "        # \"day_of_week\": X_new_encoded[\"day_of_week\"].values,\n",
    "        # \"time_periods\": X_new_encoded[\"time_periods\"].values,\n",
    "    }\n",
    ")\n",
    "\n",
    "results_df[results_df['Predicted'] < 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = mean_absolute_error(results_df[\"Actual\"], results_df[\"Predicted\"])\n",
    "mae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try to find Feature importance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Assuming X is your DataFrame\n",
    "X_values = X_encoded.values\n",
    "\n",
    "# Reshape to add a time step dimension\n",
    "X_values_reshaped = X_values.reshape((X_values.shape[0], 1, X_values.shape[1]))\n",
    "\n",
    "# Convert to TensorFlow tensor\n",
    "X_tensor = tf.convert_to_tensor(X_values_reshaped, dtype=tf.float32)\n",
    "\n",
    "# Now you can use X_tensor with tape.watch()\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(X_tensor)\n",
    "    y_pred = model(X_tensor)\n",
    "\n",
    "# Compute gradients\n",
    "gradients = tape.gradient(y_pred, X_tensor)\n",
    "\n",
    "# Compute average impact\n",
    "average_impact = tf.reduce_mean(tf.abs(gradients), axis=0).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_gradients = gradients / tf.reduce_max(tf.abs(gradients))\n",
    "normalized_gradients.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming normalized_gradients is a TensorFlow tensor\n",
    "normalized_gradients_np = normalized_gradients.numpy().reshape(-1, 8)\n",
    "\n",
    "plt.bar(features, np.mean(normalized_gradients_np, axis=0))\n",
    "plt.xlabel(\"Feature Index\")\n",
    "plt.ylabel(\"Average Normalized Gradient\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
